{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb-85_dw-ot8"
      },
      "source": [
        "# Exercise 2: Detection of liver tumors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3z3DmtESlVf4"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet celluloid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oLL5LCU7w6_Z"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet torchio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bxRjSZqb6jkv"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-kfYoTnJjZjN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tarfile\n",
        "import re\n",
        "\n",
        "from celluloid import Camera\n",
        "from IPython.display import HTML\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import torchio as tio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.transforms import Grayscale\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0WB0pMZNeP_"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ9HrYRh_tKb"
      },
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttY_OaUNKMjX",
        "outputId": "4b592342-fffc-4bf2-cbad-579fe37782ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/158PtvUr8lfOwjSGt_oaovb-hZuMvQxOl/AI_for_Healthcare/Exercise2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "%cd /content/drive/MyDrive/AI_for_Healthcare/Exercise2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6x40k6YiLAnO"
      },
      "outputs": [],
      "source": [
        "# tar_path = 'Task03_Liver.tar'\n",
        "\n",
        "# with tarfile.open(tar_path, 'r') as tar:\n",
        "#     tar.extractall()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BbrKKTAajUx7"
      },
      "outputs": [],
      "source": [
        "root = Path(\"Task03_Liver/imagesTr/\")\n",
        "label = Path(\"Task03_Liver/labelsTr/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aolTSuc4peX1"
      },
      "outputs": [],
      "source": [
        "def img_path_to_label_path(path):\n",
        "    \"\"\"\n",
        "    replace 'imagesTr' with 'labelsTr' in the path\n",
        "    and get the subject id from 'imagesTr' for later use to find corresponding label in 'labelsTr'\n",
        "    \"\"\"\n",
        "    parts = list(path.parts)\n",
        "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"\n",
        "    label_path = Path(*parts)\n",
        "\n",
        "    # get subject id from filename\n",
        "    number_match = re.search(r'liver_(\\d+)\\.nii\\.gz', str(label_path))\n",
        "    if number_match:\n",
        "        number = int(number_match.group(1))\n",
        "        return number, label_path\n",
        "    else:\n",
        "        return None, label_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MySubject(tio.Subject):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    # Add or override any methods as needed\n",
        "    def check_consistent_attribute(\n",
        "        self,\n",
        "        attribute: str,\n",
        "        relative_tolerance: float = 1e-6,\n",
        "        absolute_tolerance: float = 1e-6,\n",
        "        message: Optional[str] = None,\n",
        "    ) -> None:\n",
        "        pass\n",
        "\n",
        "    def check_consistent_spatial_shape(self) -> None:\n",
        "        pass\n",
        "    def check_consistent_orientation(self) -> None:\n",
        "        pass\n",
        "    def check_consistent_affine(self) -> None:\n",
        "        pass"
      ],
      "metadata": {
        "id": "lBSCVKjDCVZE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TXtBN9KFq4pW"
      },
      "outputs": [],
      "source": [
        "ct_folder = Path(\"Task03_Liver/imagesTr/\")\n",
        "\n",
        "subjects_paths = list(ct_folder.glob(\"liver_*\"))\n",
        "subjects = []\n",
        "\n",
        "for subject_path in subjects_paths:\n",
        "    label_path = img_path_to_label_path(subject_path)[1]\n",
        "    subject = tio.Subject({\"CT\":tio.ScalarImage(subject_path), \"Label\":tio.LabelMap(label_path)})\n",
        "    subjects.append(subject)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tio.Subject.relative_attribute_tolerance = 1000 # applies to all instances since it is a static attribute\n",
        "tio.Subject.absolute_attribute_tolerance = 1000"
      ],
      "metadata": {
        "id": "XGf2oJu77tSe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAvN9t0o2HEo",
        "outputId": "8862b956-1dc7-4121-f9fc-2784af13eed5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('R', 'A', 'S')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "subjects[1]['CT'].orientation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "88eUq7WxxxMg"
      },
      "outputs": [],
      "source": [
        "for subject in subjects:\n",
        "    assert subject[\"CT\"].orientation == (\"R\", \"A\", \"S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOVCBKzq4BBV"
      },
      "source": [
        "All of our subjects have the same CT orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zfMTt4bi4wcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57550adc-051e-4b75-b951-9fb7341360fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3sXpV2z_zPs"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "maRuSQi2_7Yt"
      },
      "outputs": [],
      "source": [
        "preprocess = tio.Compose([\n",
        "            tio.ToCanonical(),\n",
        "            tio.CropOrPad((256, 256, 200)), # crop/pad to this shape so all samples same size\n",
        "            tio.RescaleIntensity((-1, 1)), # normalize intensity range to -1 to 1\n",
        "            tio.Clamp(out_min= -150, out_max= 250), # HU-value clipping\n",
        "            tio.Resample('CT')\n",
        "            ])\n",
        "\n",
        "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
        "\n",
        "train_transform = tio.Compose([preprocess, augmentation])\n",
        "val_transform = preprocess\n",
        "test_transform = preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtuKI5qO_z8M"
      },
      "source": [
        "## Split the data set into training and testing sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxoMZn8a4W3N"
      },
      "source": [
        "We have a total of 131 subjects. We use 105 subjects for training, and 13 subjects for each of the val and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4m2wBa5d3kEg"
      },
      "outputs": [],
      "source": [
        "# train_dataset = tio.SubjectsDataset(subjects[:105], transform=train_transform)\n",
        "# val_dataset = tio.SubjectsDataset(subjects[105:118], transform=val_transform)\n",
        "# test_dataset = tio.SubjectsDataset(subjects[118:], transform=test_transform)\n",
        "\n",
        "train_dataset = tio.SubjectsDataset(subjects[:3], transform=train_transform)\n",
        "val_dataset = tio.SubjectsDataset(subjects[3:5], transform=val_transform)\n",
        "test_dataset = tio.SubjectsDataset(subjects[5:7], transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qx8sYRd5PWu"
      },
      "source": [
        "Process the data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xSnPySPE5I-o"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Z_V_UG_9py"
      },
      "source": [
        "# Training the RetinaNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z3-gyGt7S71"
      },
      "source": [
        "Note for evaluation:\n",
        "Individual lesions are defined as 3D connected components within an image. A lesion is considered\n",
        "detected if the predicted lesion has sufficient overlap with its corresponding reference lesion, measured\n",
        "as the intersection over the union of their respective segmentation masks. It allows for a count of true\n",
        "positive, false positive, and false-negative detection, from which we compute the precision and recall of\n",
        "lesion detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Lp9b1yAJqjWB"
      },
      "outputs": [],
      "source": [
        "def mask_to_bbox(masks):\n",
        "    batch_boxes = []\n",
        "    for mask in masks:\n",
        "        image_boxes = []\n",
        "        image_labels = []\n",
        "        for label_value in torch.unique(mask):\n",
        "            if label_value == 0:  # skip the background label\n",
        "                continue\n",
        "            indices = torch.nonzero(mask == label_value, as_tuple=True)\n",
        "            if len(indices[0]) > 0:  # check if the indices tensor is not empty\n",
        "                x_min, x_max = torch.min(indices[1]), torch.max(indices[1])\n",
        "                y_min, y_max = torch.min(indices[0]), torch.max(indices[0])\n",
        "\n",
        "                # make sure bbox has positive height and width\n",
        "                if x_max > x_min and y_max > y_min:\n",
        "                    # 2D bounding box has format [y_min, x_min, y_max, x_max]\n",
        "                    bounding_box = torch.tensor([x_min.item(), y_min.item(), x_max.item(), y_max.item()], dtype=torch.float32)\n",
        "\n",
        "                    label_value = label_value.item()\n",
        "\n",
        "                    image_boxes.append(bounding_box)\n",
        "                    image_labels.append(label_value)\n",
        "\n",
        "        if len(image_boxes) == 0:\n",
        "            # add empty box if there are no objects in slice\n",
        "            batch_boxes.append({'boxes': torch.zeros((0,4),dtype=torch.float32).to(device), 'labels': torch.tensor([0], dtype=torch.int64).to(device)})\n",
        "        else:\n",
        "            image_boxes = torch.stack(image_boxes).to(device)\n",
        "            image_labels = torch.tensor(image_labels, dtype=torch.int64).to(device)\n",
        "            batch_boxes.append({'boxes': image_boxes, 'labels': image_labels})\n",
        "\n",
        "    return batch_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd4pSc4QPBNq"
      },
      "source": [
        "Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_WW8l74QehQ",
        "outputId": "f28e0085-6b6d-4cf8-8943-293a320825e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
        "\n",
        "num_classes = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 5\n",
        "\n",
        "model = retinanet_resnet50_fpn_v2(pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'Checkpoints/'"
      ],
      "metadata": {
        "id": "a3esYJlVnHyl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1m3V2YPFbVJ",
        "outputId": "6e330101-cd20-4817-d72e-2495def6628a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 3/3 [02:38<00:00, 52.70s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Train Loss: 28.6641\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 2/2 [01:01<00:00, 30.54s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Validation Loss: 530.4664\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 3/3 [02:37<00:00, 52.39s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], Train Loss: 1269.6610\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 2/2 [01:01<00:00, 30.69s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], Validation Loss: 25172.8809\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 3/3 [02:36<00:00, 52.21s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5], Train Loss: 1057.1961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 2/2 [01:01<00:00, 30.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5], Validation Loss: 12011.6842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 3/3 [02:35<00:00, 51.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5], Train Loss: 595.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 2/2 [01:00<00:00, 30.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5], Validation Loss: 8955.6092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 3/3 [02:34<00:00, 51.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Train Loss: 777.2659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 2/2 [00:59<00:00, 29.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Validation Loss: 66561.5092\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "\n",
        "    # iterate over the training data\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "\n",
        "        # get the input and target tensors for the batch\n",
        "        inputs = batch['CT']['data']\n",
        "        targets = batch['Label']['data']\n",
        "\n",
        "        batch_size, channels, height, width, depth = inputs.size()\n",
        "\n",
        "        # convert to 3 channel input to work with 2d retinanet model\n",
        "        inputs = inputs.expand(batch_size, 3, height, width, depth)\n",
        "        targets = targets.expand(batch_size, 3, height, width, depth)\n",
        "\n",
        "        # reshape inputs into required size\n",
        "        inputs = inputs.permute(0, 4, 1, 2, 3)\n",
        "        inputs = inputs.flatten(0,1)\n",
        "        inputs = [tensor.squeeze(1).to(device) for tensor in inputs]\n",
        "\n",
        "        targets = targets.permute(0, 4, 1, 2, 3)\n",
        "        targets = targets.flatten(0,1)\n",
        "        targets = [tensor.squeeze(1) for tensor in targets]\n",
        "\n",
        "        # convert the GT segmentation masks to bounding boxes\n",
        "        ground_truth_boxes = mask_to_bbox(targets)\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        split_size = 10\n",
        "        inputs = [inputs[i:i+split_size] for i in range(0, len(inputs), split_size)]\n",
        "        ground_truth_boxes = [ground_truth_boxes[i:i+split_size] for i in range(0, len(ground_truth_boxes), split_size)]\n",
        "\n",
        "        for mini_inputs, mini_gt_boxes in zip(inputs, ground_truth_boxes):\n",
        "\n",
        "            output_dict = model(mini_inputs, mini_gt_boxes)\n",
        "            loss_cls = output_dict['classification']\n",
        "            loss_reg = output_dict['bbox_regression']\n",
        "            loss = loss_cls + loss_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # calculate loss per slice and add to total\n",
        "            train_loss += loss.item() * len(mini_inputs)\n",
        "\n",
        "            del mini_inputs, mini_gt_boxes, output_dict, loss_cls, loss_reg\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # calculate the average training loss for the epoch\n",
        "    train_loss = train_loss / len(train_dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}')\n",
        "\n",
        "    # Validation loop\n",
        "    # model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "\n",
        "            # get the input and target tensors for the batch\n",
        "            inputs = batch['CT']['data']\n",
        "            targets = batch['Label']['data']\n",
        "\n",
        "            batch_size, channels, height, width, depth = inputs.size()\n",
        "\n",
        "            # convert to 3 channel input to work with 2d retinanet model\n",
        "            inputs = inputs.expand(batch_size, 3, height, width, depth)\n",
        "            targets = targets.expand(batch_size, 3, height, width, depth)\n",
        "\n",
        "            # reshape inputs into required size\n",
        "            inputs = inputs.permute(0, 4, 1, 2, 3)\n",
        "            inputs = inputs.flatten(0,1)\n",
        "            inputs = [tensor.squeeze(1).to(device) for tensor in inputs]\n",
        "\n",
        "            targets = targets.permute(0, 4, 1, 2, 3)\n",
        "            targets = targets.flatten(0,1)\n",
        "            targets = [tensor.squeeze(1) for tensor in targets]\n",
        "\n",
        "            # convert the GT segmentation masks to bounding boxes\n",
        "            ground_truth_boxes = mask_to_bbox(targets)\n",
        "\n",
        "            split_size = 10\n",
        "            inputs = [inputs[i:i+split_size] for i in range(0, len(inputs), split_size)]\n",
        "            ground_truth_boxes = [ground_truth_boxes[i:i+split_size] for i in range(0, len(ground_truth_boxes), split_size)]\n",
        "\n",
        "            for mini_inputs, mini_gt_boxes in zip(inputs, ground_truth_boxes):\n",
        "\n",
        "                output_dict = model(mini_inputs, mini_gt_boxes)\n",
        "                loss_cls = output_dict['classification']\n",
        "                loss_reg = output_dict['bbox_regression']\n",
        "                loss = loss_cls + loss_reg\n",
        "\n",
        "                # calculate loss per slice and add to total\n",
        "                val_loss += loss.item() * len(mini_inputs)\n",
        "\n",
        "                del mini_inputs, mini_gt_boxes, output_dict, loss_cls, loss_reg\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # calculate the average validation loss for the epoch\n",
        "        val_loss = val_loss / len(val_dataset)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        torch.save(model.state_dict(), f'{checkpoint_path}/model_epoch{epoch+1}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "iiUaMxm6leLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = retinanet_resnet50_fpn_v2(pretrained=True)\n",
        "# model.load_state_dict(torch.load('Checkpoing/model_epoch5.pth'))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "mCNxok0OcCG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(boxA, boxB):\n",
        "    if len(boxA) == 0 or len(boxB) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # calculate IoU\n",
        "    iou = ops.box_iou(boxA, boxB)\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "JpERmmbzyZxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "total_iou = 0\n",
        "total_tumors = 0\n",
        "correct_tumors = 0\n",
        "tp = 0\n",
        "tn = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc='Testing'):\n",
        "\n",
        "        # get the input and target tensors for the batch\n",
        "        inputs = batch['CT']['data']\n",
        "        targets = batch['Label']['data']\n",
        "\n",
        "        batch_size, channels, height, width, depth = inputs.size()\n",
        "\n",
        "        # convert to 3 channel input to work with 2d retinanet model\n",
        "        inputs = inputs.expand(batch_size, 3, height, width, depth)\n",
        "        targets = targets.expand(batch_size, 3, height, width, depth)\n",
        "\n",
        "        # reshape inputs into required size\n",
        "        inputs = inputs.permute(0, 4, 1, 2, 3)\n",
        "        inputs = inputs.flatten(0,1)\n",
        "        inputs = [tensor.squeeze(1).to(device) for tensor in inputs]\n",
        "\n",
        "        targets = targets.permute(0, 4, 1, 2, 3)\n",
        "        targets = targets.flatten(0,1)\n",
        "        targets = [tensor.squeeze(1) for tensor in targets]\n",
        "\n",
        "        # convert the GT segmentation masks to bounding boxes\n",
        "        ground_truth_boxes = mask_to_bbox(targets)\n",
        "\n",
        "        split_size = 10\n",
        "        inputs = [inputs[i:i+split_size] for i in range(0, len(inputs), split_size)]\n",
        "        ground_truth_boxes = [ground_truth_boxes[i:i+split_size] for i in range(0, len(ground_truth_boxes), split_size)]\n",
        "\n",
        "        for mini_inputs, mini_gt_boxes in zip(inputs, ground_truth_boxes):\n",
        "\n",
        "            output_dicts = model(mini_inputs, mini_gt_boxes)\n",
        "            # loss_cls = output_dict['classification']\n",
        "            # loss_reg = output_dict['bbox_regression']\n",
        "            # loss = loss_cls + loss_reg\n",
        "\n",
        "            # # calculate loss per slice and add to total\n",
        "            # test_loss += loss.item() * len(mini_inputs)\n",
        "\n",
        "            for output_dict, gt_box in zip(output_dicts, mini_gt_boxes):\n",
        "                # Get the predicted boxes, labels, and scores\n",
        "                predicted_boxes = output_dict['boxes']\n",
        "                predicted_labels = output_dict['labels']\n",
        "                predicted_scores = output_dict['scores']\n",
        "\n",
        "                for box, label in zip(predicted_boxes, predicted_labels):\n",
        "                    # classify tumors\n",
        "                    iou = calculate_iou(gt_box['boxes'], box)\n",
        "                    if torch.is_tensor(iou):\n",
        "                        is_tumor = (iou > 0.5).any().item()\n",
        "                    else:\n",
        "                        is_tumor = iou > 0.5\n",
        "\n",
        "\n",
        "                    if is_tumor:\n",
        "                        total_tumors += 1\n",
        "                        if predicted_labels.item() == 1:  # Assuming tumor class is 1\n",
        "                            correct_tumors += 1\n",
        "\n",
        "                    if is_tumor and gt_box['labels'].item() == 2:\n",
        "                        tp += 1\n",
        "                    elif is_tumor and (gt_box['labels'].item() == 0 or gt_box['labels'].item() == 1):\n",
        "                        fn += 1\n",
        "                    elif not is_tumor and (gt_box['labels'].item() == 0 or gt_box['labels'].item() == 1):\n",
        "                        fp += 1\n",
        "                    elif not is_tumor and (gt_box['labels'].item() == 0):\n",
        "                        tn += 1\n",
        "\n",
        "            del mini_inputs, mini_gt_boxes, output_dict\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # calculate the average test loss for the epoch\n",
        "    test_loss = test_loss / len(val_dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}')\n",
        "\n",
        "# calculate metrics\n",
        "accuracy = correct_tumors / total_tumors\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "test_loss = test_loss / len(test_dataset)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1_score:.4f}')\n"
      ],
      "metadata": {
        "id": "3XUzZ7tUmcrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}